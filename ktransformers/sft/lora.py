import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq
from transformers import Trainer, TrainingArguments
from transformers.trainer import TRAINING_ARGS_NAME
import torch
from torch.utils.data import DataLoader
from datasets import Dataset
from peft import LoraConfig, TaskType
import os
from torchviz import make_dot
from torch.profiler import profile, record_function, ProfilerActivity
import torch.nn.functional as F
from transformers import TrainerCallback
import gc
from tqdm import tqdm
import os, torch, json, tempfile
from pathlib import Path
from accelerate import Accelerator

from ktransformers.sft.peft_utils.mapping import inject_adapter_in_model, get_peft_model
from ktransformers.sft.peft_utils.lora_layer import KTransformersLinearLora
from ktransformers.sft.flops_utils.custom_profile import custom_profile
from ktransformers.operators.experts import KExpertsTorch, KTransformersExperts
# from ktransformers.sft.load_lora import get_custom_peft_model

# FOR: not A or H GPU
os.environ["NCCL_P2P_DISABLE"]  = "1"
os.environ["NCCL_IB_DISABLE"]  = "1"

layer_data = {}  # å­˜å‚¨å„å±‚è¾“å…¥è¾“å‡ºæ•°æ®

def record_layer_io(module, input, output, layer_name):
    layer_data[layer_name] = {
        'input': input[0].detach().clone(),
        'output': output.detach().clone()
    }

# æ³¨å†Œé’©å­
hooks = []
target_layers = [
    'base_model.model.model.orig_module.layers.0.self_attn.kv_a_proj_with_mqa',
    'base_model.model.model.orig_module.layers.0.self_attn.kv_b_proj',
    # 'base_model.model.model.orig_module.layers.1.self_attn.kv_a_proj_with_mqa',
    # 'base_model.model.model.orig_module.layers.1.self_attn.kv_b_proj'
]


# è‡ªå®šä¹‰å›è°ƒï¼Œæ‰‹åŠ¨æ§åˆ¶Profilerï¼Œé¿å…transformeråº“ç‰ˆæœ¬å¤ªä½
class ProfilerCallback(TrainerCallback):
    def __init__(self, profiler):
        self.profiler = profiler

    def on_step_end(self, args, state, control, **kwargs):
        self.profiler.step()

def preprocess_function(batch, tokenizer, max_len=512):
    full_inputs = [ins + inp for ins, inp in zip(batch["instruction"], batch["input"])]
    
    # print(f"FI: {full_inputs}")
    # print(f"TFI: {type(full_inputs)}")
    tokenized_inputs = tokenizer(full_inputs, padding="max_length", truncation=True, max_length=max_len)
    tokenized_outputs = tokenizer(batch["output"], padding="max_length", truncation=True, max_length=max_len)

    # need batch=false, just for debug and test
    # print("ğŸ”¹Instruction tokens:", len(tokenizer.tokenize(instruction)))
    # print("ğŸ”¹Input tokens:", len(tokenizer.tokenize(inputs)))
    # print("ğŸ”¹Instruction+Input tokens:", len(tokenizer.tokenize(full_input)))
    # print("ğŸ”¸Output tokens (label):", len(tokenizer.tokenize(targets)))

    tokenized_inputs["labels"] = tokenized_outputs["input_ids"]
    return tokenized_inputs

class KTrainer(Trainer):
    def save_model(self, output_dir=None, _internal_call=False):
        output_dir = output_dir or self.args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        # åªä¿å­˜ LoRA adapterï¼ˆå« adapter_config.jsonï¼‰
        self.model.save_pretrained(output_dir)
        
    def _move_model_to_device(self, model, device):
        print("[KTrainer] Due to the placement feature in KTransformers, skip moving model to", device)
        return model

def inspect_device(model, write_file):
    for name, module in model.named_modules(): 
        with open(write_file, 'a') as file:
            file.write(f"Layer: {name}\n")
        # print(f"Layer: {name}")
        # æ£€æŸ¥å‚æ•° 
        for param_name, param in module.named_parameters(recurse=False): 
            with open(write_file, 'a') as file:
                file.write(f"  Parameter '{param_name}' device: {param.device}\n")
            # print(f"  Parameter '{param_name}' device: {param.device}") 
        # æ£€æŸ¥ç¼“å†²åŒºï¼ˆå¦‚BatchNormçš„running_meanï¼‰
        for buffer_name, buffer in module.named_buffers(recurse=False): 
            with open(write_file, 'a') as file:
                file.write(f"  Buffer '{buffer_name}' device: {buffer.device}\n")
            # print(f"  Buffer '{buffer_name}' device: {buffer.device}")

def print_model_params(model):
    # éå†æ‰€æœ‰Decoderå±‚ï¼ˆå…±27å±‚ï¼‰
    # for layer_idx in range(len(model.model.orig_module.layers)):
    for layer_idx in range(0, 3):
        layer = model.model.orig_module.layers[layer_idx]
        
        # ============= æ‰“å°æ³¨æ„åŠ›å±‚å‚æ•° =============
        print(f"\n================ Layer {layer_idx} Attention ================")
        
        # æ‰“å°q_projå‚æ•°
        q_proj = layer.self_attn.orig_module.q_proj.orig_module
        print(f"\nq_proj.generate_linear.weight (shape: {q_proj.generate_linear.weight.shape})")
        print(q_proj.generate_linear.weight.cpu())
        
        # # æ‰“å°kv_a_projå‚æ•°
        # kv_a_proj = layer.self_attn.orig_module.kv_a_proj_with_mqa.orig_module
        # print(f"\nkv_a_proj.weight (shape: {kv_a_proj.weight.shape})")
        # print(kv_a_proj.weight.data[:3, :5].detach().cpu().numpy())
        
        # # æ‰“å°o_projå‚æ•°
        # o_proj = layer.self_attn.orig_module.o_proj.orig_module
        # print(f"\no_proj.weight (shape: {o_proj.weight.shape})")
        # print(o_proj.weight.data[:3, :5].detach().cpu().numpy())
        
        # # ============= æ‰“å°MLP/MoEå‚æ•° =============
        # print(f"\n================ Layer {layer_idx} MLP/MoE ================")
        
        # # åŒºåˆ†æ™®é€šMLPå’ŒMoEå±‚ï¼ˆç¬¬0å±‚æ˜¯æ™®é€šMLPï¼Œå…¶ä»–æ˜¯MoEï¼‰
        # if layer_idx == 0:
        #     # æ™®é€šMLPå±‚å‚æ•°
        #     mlp = layer.mlp
        #     for proj_type in ['gate_proj', 'up_proj', 'down_proj']:
        #         module = getattr(mlp, proj_type).orig_module
        #         print(f"\n{proj_type}.weight (shape: {module.weight.shape})")
        #         print(module.weight.data[:3, :5].detach().cpu().numpy())
        # else:
        #     # MoEå±‚å‚æ•°
        #     moe = layer.mlp.orig_module
        #     # æ‰“å°å…±äº«ä¸“å®¶å‚æ•°
        #     print("\n[Shared Experts]")
        #     for proj_type in ['gate_proj', 'up_proj', 'down_proj']:
        #         module = getattr(moe.shared_experts, proj_type).orig_module
        #         print(f"\nshared_{proj_type}.weight (shape: {module.weight.shape})")
        #         print(module.weight.data[:3, :5].detach().cpu().numpy())
            
        #     # æ‰“å°64ä¸ªä¸“å®¶å‚æ•°ï¼ˆé‡‡æ ·å‰3ä¸ªï¼‰
        #     print("\n[Experts]")
        #     for expert_idx in range(3):  # é‡‡æ ·å‰3ä¸ªä¸“å®¶
        #         expert = moe.experts.orig_module[expert_idx]
        #         print(f"\nExpert {expert_idx}:")
        #         for proj_type in ['gate_proj', 'up_proj', 'down_proj']:
        #             module = getattr(expert, proj_type)
        #             print(f"{proj_type}.weight (shape: {module.weight.shape})")
        #             print(module.weight.data[:3, :5].detach().cpu().numpy())

def print_lora_params(model):
    # éå†æ‰€æœ‰Decoderå±‚ (ç´¢å¼•0åˆ°26å…±27å±‚)
    # for layer_idx in range(len(model.model.orig_module.layers)):
    for layer_idx in range(0, 3):
        # è·å–å½“å‰Decoderå±‚
        layer = model.base_model.model.model.orig_module.layers[layer_idx]
        # layer = model.model.orig_module.layers[layer_idx]
        
        # å®šä½åˆ°ç›®æ ‡æ¨¡å—è·¯å¾„
        q_proj_module = layer.self_attn.orig_module.q_proj.orig_module
        
        # æå–ç›®æ ‡çŸ©é˜µå‚æ•°
        linear_weight = q_proj_module.generate_linear.weight
        lora_A_weight = q_proj_module.lora_A["default"].weight
        lora_B_weight = q_proj_module.lora_B["default"].weight
        
        # æ‰“å°å‚æ•°ä¿¡æ¯
        print(f"\n=================== Layer {layer_idx} ===================")
        
        # æ‰“å°åŸLinearçŸ©é˜µå‚æ•°
        print("\nOriginal Linear (first row slice):")
        print(linear_weight.cpu())  # ç¬¬ä¸€è¡Œå‰5ä¸ªå‚æ•°
        
        # æ‰“å°Lora_Aå‚æ•°
        print("\nLora_A (first row slice):")
        print(lora_A_weight.cpu())  # ç¬¬ä¸€è¡Œå‰5ä¸ªå‚æ•°
        
        # æ‰“å°Lora_Bå‚æ•°
        print("\nLora_B (first row slice):")
        print(lora_B_weight.cpu())  # ç¬¬ä¸€è¡Œå‰5ä¸ªå‚æ•°

def print_grad_fn(grad_fn, indent=0):
    """é€’å½’æ‰“å°è®¡ç®—å›¾èŠ‚ç‚¹"""
    if grad_fn is None:
        return
    # æ‰“å°å½“å‰èŠ‚ç‚¹ä¿¡æ¯
    print(' ' * indent, f"Node: {str(grad_fn).split('(')[0]}")
    print(' ' * indent, f"  Metadata: {grad_fn.metadata}")
    # éå†å­èŠ‚ç‚¹
    for child in getattr(grad_fn, 'next_functions', []):
        if child[0] is not None:
            print_grad_fn(child[0], indent + 2)

def forward_hook(module, inputs, output):
    if isinstance(output, (tuple, list)):
        for i, o in enumerate(output):
            if o is None:
                print(f"{module.__class__.__name__} output index {i} is None")
            else:
                print(f"{module.__class__.__name__} output index {i}: requires_grad={o.requires_grad}, grad_fn={o.grad_fn}")
    elif output is None:
        print(f"{module.__class__.__name__} returned None")
    else:
        print(f"{module.__class__.__name__}: requires_grad={output.requires_grad}, grad_fn={output.grad_fn}")

def check_moe_gradients(model):
    moe_layer = model.base_model.model.model.orig_module.layers[1].mlp.orig_module
    for name, param in moe_layer.named_parameters():
        if param.requires_grad and param.grad is not None:
            grad_norm = torch.norm(param.grad)
            print(f"MoEå‚æ•° {name} æ¢¯åº¦èŒƒæ•°: {grad_norm}")
        else:
            print(f"MoEå‚æ•° {name} æ— æ¢¯åº¦")

def disable_all_dropout(module):
        for name, child in module.named_children():
            if isinstance(child, torch.nn.Dropout):
                child.p = 0  # ç›´æ¥ä¿®æ”¹æ¦‚ç‡å‚æ•°
                child.inplace = False  # ç¡®ä¿ä¸å½±å“åŸå§‹æ•°æ®
            disable_all_dropout(child)  # é€’å½’å¤„ç†å­æ¨¡å—

def verify_lora_layers(model):
    for layer_path in target_layers:
        # è·å–æ¨¡å—å®ä¾‹
        module = model.get_submodule(layer_path)
        orig_module = module.orig_module
        
        # æå–å‚æ•°
        W = orig_module.weight.data  # [576, 2048] -> [2048, 576]
        lora_A = module.lora_A['default'].weight.data  # [8, 2048]
        lora_B = module.lora_B['default'].weight.data  # [576, 8]
        alpha_over_r = 32/8  # alpha=32, r=8
        
        # è·å–è®°å½•çš„æ•°æ®ï¼ˆä¿æŒbatchç»´åº¦ï¼‰
        input_tensor = layer_data[layer_path]['input']  # [1, 512, 2048]
        
        # æ‰‹åŠ¨è®¡ç®—æµç¨‹
        # åŸå§‹éƒ¨åˆ†è®¡ç®—
        try:
            original_output = torch.matmul(input_tensor, W)  # [1,512,2048] @ [2048,576] => [1,512,576]
        except:
            original_output = torch.matmul(input_tensor, W.T)  # [1,512,2048] @ [2048,576] => [1,512,576]
        
        # LoRAéƒ¨åˆ†è®¡ç®—
        lora_effect = torch.matmul(
            torch.matmul(input_tensor, lora_A.T),  # [1,512,2048] @ [2048,8] => [1,512,8]
            lora_B.T  # [1,512,8] @ [8,576] => [1,512,576]
        ) * alpha_over_r
        
        # åˆå¹¶ç»“æœ
        manual_output = original_output + lora_effect  # [1,512,576]
        
        # è·å–æ¨¡å‹è¾“å‡º
        model_output = layer_data[layer_path]['output']

        print(f"manual_output:{manual_output}")
        print(f"model_output:{model_output}")
        
        # æ•°å€¼æ¯”è¾ƒ
        if torch.allclose(manual_output, model_output, atol=1e-5):
            print(f"{layer_path} éªŒè¯é€šè¿‡")
        else:
            print(f"{layer_path} éªŒè¯å¤±è´¥ï¼æœ€å¤§è¯¯å·®ï¼š{torch.max(torch.abs(manual_output - model_output))}")

def print_moe_stats(moe_layer: KExpertsTorch):
    print(f"Total Params: {moe_layer.total_params/1e6:.2f}M")
    
    total_time = sum(moe_layer.times)
    gflops = (moe_layer.total_flops / 1e9) / total_time if total_time !=0 else 0
    
    print(f"Total Calls: {moe_layer.call_count}")
    # print(f"Avg GFLOPS per Call: {gflops/moe_layer.call_count:.2f}")
    print(f"Overall GFLOPS: {gflops:.2f}")
    
    # æ‰“å°å•æ¬¡è°ƒç”¨ç¤ºä¾‹
    if moe_layer.call_count > 0:
        last_flops = moe_layer.flops_per_call[-1]
        last_time = moe_layer.times[-1]
        print(f"\nLast Call - FLOPs: {last_flops/1e9:.2f}G  Time: {last_time*1000:.2f}ms  "
              f"GFLOPS: {(last_flops/1e9)/last_time:.2f}")
        
def recursive_traverse(model, parent_name=''):
    """
    é€’å½’éå†æ¨¡å‹ï¼ŒæŸ¥æ‰¾MoEå±‚å¹¶è°ƒç”¨print_moe_statsã€‚
    """
    # éå†æ¨¡å‹ä¸­çš„æ‰€æœ‰å­æ¨¡å—
    for name, module in model.named_children():
        full_name = f"{parent_name}.{name}" if parent_name else name
        
        # å¦‚æœæ˜¯ MoE å±‚ï¼Œè°ƒç”¨ print_moe_stats
        if isinstance(module, KTransformersExperts):  # æ£€æŸ¥æ˜¯å¦ä¸º MoE å±‚
            print(f"Found MoE layer: {full_name}")
            print_moe_stats(module.generate_experts)
        
        # é€’å½’å¤„ç†å­æ¨¡å—
        recursive_traverse(module, full_name)

def log_step_state(
    step: int,
    inputs: dict,
    loss: torch.Tensor,
    model: torch.nn.Module,
    log_dir: str = "train_logs",
):
    """
    æŠŠå½“å‰ step çš„è¾“å…¥ / loss / grad / param ä¿å­˜åˆ° log_dir/step_{step}.pt
    """
    Path(log_dir).mkdir(parents=True, exist_ok=True)

    # â‘  å¤„ç†è¾“å…¥ï¼šåªä¿å­˜å¼ é‡ï¼Œä¸”å…ˆæ¬åˆ° CPUï¼Œé¿å… GPU è¿›ç¨‹é—´åºåˆ—åŒ–é—®é¢˜
    logged_inputs = {
        k: v.detach().cpu()
        for k, v in inputs.items()
        if isinstance(v, torch.Tensor)
    }

    # â‘¡ loss ä¸€èˆ¬æ˜¯æ ‡é‡ Tensor
    loss_val = loss.detach().cpu()

    # â‘¢ å‚æ•°ä¸æ¢¯åº¦
    params, grads = {}, {}
    for name, p in model.named_parameters():
        params[name] = p.detach().cpu()
        grads[name] = p.grad.detach().cpu() if p.grad is not None else None

    torch.save(
        {
            "step": step,
            "inputs": logged_inputs,
            "loss": loss_val,
            "params": params,
            "grads": grads,
        },
        f"{log_dir}/step_{step:08d}.pt",
    )

def collect_gradients(model, input_ids):
    # ç¡®ä¿å¯å¤ç°æ€§
    torch.manual_seed(42)
    
    output = model(input_ids=input_ids)
    
    logits = output.logits
    loss = logits.mean()
    
    # åå‘ä¼ æ’­
    model.zero_grad()
    loss.backward()
    
    # æ”¶é›†æ¢¯åº¦ä¿¡æ¯
    grads = []
    for name, param in model.named_parameters():
        if param.requires_grad and param.grad is not None:
            grads.append(f"{name}: {param.grad.norm().item():.6f}")
    
    return grads

def report_meta_tensors(model):
    import torch, inspect
    meta_modules = []
    for mod_name, mod in model.named_modules():
        metas = []
        # ä»…æ£€æŸ¥å½“å‰æ¨¡å—è‡ªèº«ï¼ˆä¸é€’å½’ï¼‰æŒ‚è½½çš„å‚æ•°/ç¼“å†²
        for n, p in list(mod.named_parameters(recurse=False)):
            if getattr(p, "is_meta", False) and p.is_meta:
                metas.append(("param", n, tuple(p.shape)))
        for n, b in list(mod.named_buffers(recurse=False)):
            if getattr(b, "is_meta", False) and b.is_meta:
                metas.append(("buffer", n, tuple(b.shape)))
        if metas:
            print(f"[META] {mod_name} ({type(mod).__name__}): {metas}")
            meta_modules.append((mod_name, type(mod).__name__, metas))
    return meta_modules

def lora_and_load_adapter(model, tokenizer, sft_data_path, save_adapter_path, is_profiler=False):

    torch.autograd.set_detect_anomaly(True) # åœ¨åå‘ä¼ æ’­å‡ºé”™æ—¶ï¼ŒPyTorch ä¼šæä¾›æ›´è¯¦ç»†çš„å †æ ˆä¿¡æ¯
    
    # tokenizer = AutoTokenizer.from_pretrained('/data/model/Qwen2.5-7B-Instruct', trust_remote_code=True)

    dataset = Dataset.from_json(sft_data_path)

    processed_dataset = dataset.map(lambda  examples: preprocess_function(examples, tokenizer), batched=True)
    split_dataset = processed_dataset.train_test_split(test_size=0.1)

    train_dataset = split_dataset["train"]
    val_dataset = split_dataset["test"]

    train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False)
    # val_dataloader = DataLoader(val_dataset, batch_size=8)

    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        target_modules=[
            "q_proj",
            "kv_a_proj_with_mqa",
            "kv_b_proj",
            "o_proj"
        ],
        r=8,
        lora_alpha=32,
        lora_dropout=0.1,
    )

    training_args = TrainingArguments(
        output_dir=save_adapter_path,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=16,
        num_train_epochs=1,
        # max_steps=4, # TODO: FOR TEST, will override any value given in num_train_epochs
        learning_rate=3e-4,
        fp16=False,
        logging_steps=10,
        save_steps=1000,
        dataloader_drop_last=True,
        ddp_find_unused_parameters=False,
    )
    
    # model = inject_adapter_in_model(lora_config, model)
    model = get_peft_model(model, lora_config)
    # model = get_custom_peft_model(model, lora_config)

    model.config.use_cache = False

    model.print_trainable_parameters() 
    
    # print(f"model:{model}")
    
    # output = model(input_ids=torch.tensor([[1,2,3]], dtype=torch.int32, device="cuda:0"))
    # loss = output.logits.mean()
        
    # dot = make_dot(loss, params=dict(model.named_parameters()))
    # dot.render("KT_compute_cpuinfer_moe_model_graph", format="svg")
    
    # _ = report_meta_tensors(model)
    
    trainer = KTrainer(
        model=model,
        train_dataset=train_dataset,
        args=training_args,
        data_collator=DataCollatorForSeq2Seq(
            tokenizer, pad_to_multiple_of=8, return_tensors="pt", padding=True
        )
    )
    trainer.accelerator = Accelerator(device_placement=False)
    # first_batch = next(iter(trainer.get_train_dataloader()))
    # print("Batch keys:", list(first_batch.keys()))

    print("-------------------------START TRAINING!!!-------------------------")

    trainer.train()

    # input_ids = torch.randint(0, 1000, (32, 128), device="cuda:0")
    # gradients = collect_gradients(model, input_ids)
    
    # with open(f"/home/lpl/KT-SFT/tmp/KSFTExpertsCPU_grads.txt", "w") as f:
    #     f.write("\n".join(gradients))
    # print(xx)
    
    # -----------------æ¨¡å‹è¾“å…¥æ•°æ®æµ‹è¯•-----------------
    # total_length = 0
    # valid_count = 0
    # for batch in tqdm(train_dataloader):
    #     input_ids = batch['input_ids']
    #     # print(f"Token count per sample: {[len(ids) for ids in input_ids]}")
    #     for ids in input_ids:
    #         if not torch.equal(ids, torch.tensor([100001])):
    #             total_length += len(ids)
    #     valid_count += 1
    #     # print(f"Input tensor: {input_ids}")
    #     # print(f"total_length:{total_length}")
    #     # break

    # if valid_count > 0:
    #     average_length = total_length / valid_count
    #     print(f"å¹³å‡é•¿åº¦: {average_length}")
    # else:
    #     print("æ²¡æœ‰æœ‰æ•ˆçš„ input_ids å…ƒç´ ã€‚")

    # print(xx)
    # -----------------æ¨¡å‹è¾“å…¥æ•°æ®æµ‹è¯•-----------------
    
    # -----------------æ¨¡å‹FLOPSæµ‹è¯•ï¼ˆTHOPæ–¹æ³•ï¼‰-----------------
    # æ²¡æœ‰ç»§ç»­ä½¿ç”¨è¿™ç§æ–¹å¼è¿›è¡Œæµ‹è¯•ï¼ŒåŸå› åœ¨äºéœ€è¦å¯¹æ¯ä¸ªç¬¬ä¸‰æ–¹æ¨¡å—è¿›è¡Œæ·»åŠ ï¼ˆæ–¹æ³•æœ¬èº«ä¸è®¤ï¼‰ã€‚
    # éœ€è¦çš„è¯å¯ä»¥å‚è€ƒï¼šhttps://github.com/ultralytics/thop é‡Œé¢çš„Define Custom Rules for Third-Party Modules
    # from ktransformers.sft.flops_utils.custom_profile import custom_profile

    # for module in model.modules():
    #     if not hasattr(module, 'total_ops'):
    #         module.register_buffer('total_ops', torch.zeros(1, dtype=torch.float64))
    #     if not hasattr(module, 'total_params'):
    #         module.register_buffer('total_params', torch.zeros(1, dtype=torch.float64))
            
    # # print(f"input:{input}")
    # for inputs in tqdm(train_dataloader):
    #     # input_ids = batch['input_ids']
    #     # del inputs['instruction']
    #     # del inputs['input']
    #     # del inputs['output']
    #     # output = model(**inputs)
    #     model.eval()
    #     content = inputs['instruction'][0] + inputs['input'][0]
    #     # flops,params = custom_profile(model, inputs=inputs, content=content, tokenizer=tokenizer, custom_ops={YourModule: count_your_model})
    #     # print('FLOPs = ' + str(flops / 1000 ** 3) + 'G')
    #     # print('Params = ' + str(params / 1000 ** 2) + 'M')

    #     messages = [{"role": "user", "content": content}]
    #     input_tensor = tokenizer.apply_chat_template(
    #         messages, add_generation_prompt=True, return_tensors="pt"
    #     )
    #     with torch.no_grad():
    #         # model(*inputs)
    #         # model.model to deal with the PeftModelForCaualLM temp
    #         prefill_and_generate(
    #             model.model, tokenizer, input_tensor.cuda(), max_new_tokens=1000, use_cuda_graph=False, mode = 'normal', force_think = False, chunk_prefill_size = 8192,
    #         )
    #     recursive_traverse(model)
    # -----------------æ¨¡å‹FLOPSæµ‹è¯•ï¼ˆTHOPæ–¹æ³•ï¼‰-----------------
    
    # -----------------è®¡ç®—å›¾æµ‹è¯•-----------------
    # output = model(input_ids=torch.tensor([[1,2,3]], dtype=torch.int32, device="cuda:0"))
    # loss = output.logits.mean()
        
    # dot = make_dot(loss, params=dict(model.named_parameters()))
    # dot.render("KT_compute_cpuinfer_moe_model_graph", format="svg")
    # -----------------è®¡ç®—å›¾æµ‹è¯•-----------------

    # -----------------KSFTå‰å‘æµ‹è¯•-----------------
    # with open("tmp/output_loss_KCPU.txt", "w") as file:
    #     file.write("Output (logits):\n")
    #     file.write(str(output.logits.cpu().detach().numpy()))  # è¿™é‡Œå°†å¼ é‡è½¬æ¢ä¸º numpy æ•°ç»„åå†™å…¥
    #     file.write("\n\nLoss:\n")
    #     file.write(str(loss.item()))  # è¿™é‡Œå°† loss çš„å€¼è½¬æˆå­—ç¬¦ä¸²
    # -----------------KSFTå‰å‘æµ‹è¯•-----------------
    
    # -----------------æ¨¡å‹å±‚ç¡®å®šæ€§æ¢¯åº¦æµ‹è¯•-----------------
    # disable_all_dropout(model)

    # def print_dropout_status(module, prefix=""):
    #     for name, child in module.named_children():
    #         if isinstance(child, torch.nn.Dropout):
    #             print(f"{prefix}{name}: p={child.p}, training={child.training}")
    #         print_dropout_status(child, prefix + name + ".")
    
    # print("Dropoutå±‚çŠ¶æ€éªŒè¯ï¼š") # ç©ºè¾“å‡ºæˆ–è€…p=0å°±æ˜¯æˆåŠŸéªŒè¯
    # print_dropout_status(model)

    # for layer_path in target_layers:
    #     module = model.get_submodule(layer_path)
    #     hook = module.register_forward_hook(
    #         lambda m, i, o, ln=layer_path: record_layer_io(m, i, o, ln)
    #     )
    #     hooks.append(hook)
    # -----------------æ¨¡å‹å±‚ç¡®å®šæ€§æ¢¯åº¦æµ‹è¯•-----------------

    
    # -----------------æ¨¡å‹å±‚æ€§èƒ½åˆæ­¥æµ‹è¯•-----------------
    # if is_profiler:
    #     profiler = profile(
    #         activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    #         schedule=torch.profiler.schedule(
    #             wait=1,        # è·³è¿‡ç¬¬1æ­¥
    #             warmup=1,      # é¢„çƒ­ç¬¬2æ­¥
    #             active=1,      # ä»…è®°å½•æ¥ä¸‹æ¥3æ­¥ï¼ˆå‡å°‘æ˜¾å­˜å ç”¨ï¼‰
    #             repeat=1       # ä¸é‡å¤
    #         ),
    #         on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'),
    #         record_shapes=False,
    #         profile_memory=False, # å…³é—­å†…å­˜åˆ†æï¼Œé¿å…å ç”¨å¤§é‡å†…å­˜ï¼ˆç›®å‰è¿™ä¸ªæœåŠ¡å™¨CPUå†…å­˜ä¸æ˜¯å¾ˆå¤§ï¼‰
    #         with_stack=False
    #     )

    #     # transformerç‰ˆæœ¬ä½ä¸æ”¯æŒï¼Œä¸èƒ½ç›´æ¥åœ¨TrainingArgumentsé‡Œé¢å†™profiler_args
    #     # profiler_args = {
    #     #     "activities": [ProfilerActivity.CPU, ProfilerActivity.CUDA],  # åŒæ—¶ç›‘æ§CPUå’ŒCUDA
    #     #     "record_shapes": True,         # è®°å½•å¼ é‡å½¢çŠ¶
    #     #     "profile_memory": True,        # è®°å½•å†…å­˜æ¶ˆè€—
    #     #     "with_stack": True,            # è®°å½•è°ƒç”¨æ ˆä¿¡æ¯
    #     #     "on_trace_ready": torch.profiler.tensorboard_trace_handler('./logs'),  # è‡ªåŠ¨ä¿å­˜åˆ°TensorBoard
    #     #     "schedule": torch.profiler.schedule(
    #     #         wait=1,        # è·³è¿‡å‰1æ­¥
    #     #         warmup=1,      # é¢„çƒ­1æ­¥
    #     #         active=100,     # è®°å½•æ¥ä¸‹æ¥100æ­¥ï¼ˆè¦†ç›–å…¨éƒ¨è®­ç»ƒæ­¥ï¼‰
    #     #         repeat=1       # ä¸é‡å¤
    #     #     )
    #     # }

    #     trainer = KTrainer(
    #         model=model,
    #         train_dataset=train_dataset,
    #         args=training_args,            # ä½¿ç”¨ä¿®æ”¹åçš„å‚æ•°
    #         data_collator=DataCollatorForSeq2Seq(
    #             tokenizer, pad_to_multiple_of=8, return_tensors="pt", padding=True
    #         ),
    #         callbacks=[ProfilerCallback(profiler)]
    #     )

    #     with profiler:
    #         trainer.train()

    #     print("Training finished. Exporting profiler data...")
    #     with open("profiler_output.txt", "w") as f:
    #         f.write(profiler.key_averages().table(sort_by="cuda_time_total", row_limit=20))
    
    #   profiler.export_chrome_trace("trace.json")
    
    #   check_moe_gradients(model) # è°ƒè¯•ç»“æœï¼šæ— æ¢¯åº¦
    
    # -----------------æ¨¡å‹å±‚æ€§èƒ½åˆæ­¥æµ‹è¯•-----------------

    # verify_lora_layers(model)

    # model.save_pretrained(save_adapter_path)

    '''
    ----------------------- START: Lora Test -----------------------
    
    # print(f"LoRAå‰:{model}")

    # for name, module in model.named_modules():
    #     if "q_proj" in name or "kv_a_proj" in name or "o_proj" in name:
    #         print(name)

    # print_model_params(model)

    # model = KTransformersLinearLora()

    # inspect_device(model, '/home/yj/ktransformers/device1.txt')
    # with open('/home/yj/ktransformers/device1.txt', 'a') as file:
    #     file.write(f"Base model device: {model.base_model.device}\n")
        # file.write(f"LoRA adapter device: {model.lora_config['target_modules'].device}\n")
    # print(f"Base model device: {model.base_model.device}") 
    # print(f"LoRA adapter device: {model.lora_config['target_modules'].device}") 

    # print(f"LoRAå:{model}")

    # model = model.to('cuda')

    # for name, module in model.named_modules():
    #     module.register_forward_hook(forward_hook)

    # for name, parms in model.named_parameters():	
    #     # parms.requires_grad = True
    #     print('-->name:', name)
    #     print('-->para:', parms)
    #     print('-->grad_requirs:',parms.requires_grad)
    #     print('-->grad_fn:',parms.grad_fn)
    #     print('-->grad_value:',parms.grad)
    #     print("===")

    # # é€‰æ‹©ç‰¹å®šå±‚çš„è¾“å…¥è¾“å‡º
    # output = model(input_ids=torch.tensor([[1,2,3]], dtype=torch.int32, device="cuda:0"))
    # loss = output.logits.mean()

    # dot = make_dot(loss, params=dict(model.named_parameters()))
    # dot.render("KT_compute_graph", format="svg")

    # inspect_device(model, '/home/yj/ktransformers/device2.txt')
    # with open('/home/yj/ktransformers/device2.txt', 'a') as file:
    #     file.write(f"Base model device: {model.base_model.device}\n")
        # file.write(f"LoRA adapter device: {model.lora_config['target_modules'].device}\n")
    # print(f"Base model device: {model.base_model.device}") 
    # print(f"LoRA adapter device: {model.lora_config['target_modules'].device}") 

    # print_lora_params(model)

    # è¢«å¸¦profileçš„Traineræ›¿ä»£
    # trainer = KTrainer(
    #     model=model,
    #     train_dataset=train_dataset,
    #     args=transformers.TrainingArguments(
    #         output_dir=save_adapter_path,
    #         per_device_train_batch_size=1,
    #         gradient_accumulation_steps=16,
    #         num_train_epochs=10,
    #         learning_rate=3e-4,
    #         fp16=False,
    #         logging_steps=10,
    #         save_steps=200,
    #         # å¯é¢å¤–æ·»åŠ åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–å‚æ•° 
    #         dataloader_drop_last=True,
    #         ddp_find_unused_parameters=False 
    #     ),
    #     data_collator=DataCollatorForSeq2Seq(
    #         tokenizer, pad_to_multiple_of=8, return_tensors="pt", padding=True
    #     ),
    # )

    # model(input_ids=torch.tensor([[1,2,3]], dtype=torch.int32, device="cuda:0"))

    # trainer.train()

    # print_lora_params(model)

    # model = model.merge_and_unload()
    ----------------------- END: Lora Test -----------------------

    '''

def inject_lora_layer(model):

    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        target_modules=[
            "q_proj",
            "kv_a_proj_with_mqa",
            "kv_b_proj",
            "o_proj"
        ],
        r=8,
        lora_alpha=16,
        lora_dropout=0.0,
    )
    
    model = inject_adapter_in_model(lora_config, model)

    
